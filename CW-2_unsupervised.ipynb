{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Means\n",
    "import sklearn\n",
    "import keras\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, normalized_mutual_info_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, UpSampling2D, Activation\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "(x_train, y_train),(x_test, y_test)=tf.keras.datasets.fashion_mnist.load_data()\n",
    "x=np.concatenate((x_train, x_test))\n",
    "y=np.concatenate((y_train, y_test))\n",
    "print(x.shape)\n",
    "x=x_train.reshape((x_train.shape[0],-1))\n",
    "x=np.divide(x.astype(float),255)\n",
    "kmeans = KMeans(n_clusters = 10)\n",
    "kmeans.fit(x)\n",
    "\n",
    "def infer_cluster_labels(kmeans, actual_labels):\n",
    "    inferred_labels = {}\n",
    "    for i in range(kmeans.n_clusters):\n",
    "        labels = []\n",
    "        index = np.where(kmeans.labels_ == i)\n",
    "        labels.append(actual_labels[index])\n",
    "        if len(labels[0]) == 1:\n",
    "            counts = np.bincount(labels[0])\n",
    "        else:\n",
    "            counts = np.bincount(np.squeeze(labels))\n",
    "        if np.argmax(counts) in inferred_labels:\n",
    "            inferred_labels[np.argmax(counts)].append(i)\n",
    "        else:\n",
    "            inferred_labels[np.argmax(counts)] = [i]      \n",
    "    return inferred_labels  \n",
    "\n",
    "def infer_data_labels(X_labels, cluster_labels):\n",
    "    predicted_labels = np.zeros(len(X_labels)).astype(np.uint8)    \n",
    "    for i, cluster in enumerate(X_labels):\n",
    "        for key, value in cluster_labels.items():\n",
    "            if cluster in value:\n",
    "                predicted_labels[i] = key\n",
    "                \n",
    "    return predicted_labels\n",
    "    \n",
    "n_clusters=10\n",
    "cluster_labels = infer_cluster_labels(kmeans, y_train)\n",
    "X_clusters = kmeans.predict(x)\n",
    "predicted_labels = infer_data_labels(X_clusters, cluster_labels)\n",
    "print(predicted_labels[:20])\n",
    "print(y_train[:20])\n",
    "from sklearn import metrics\n",
    "\n",
    "def calculate_metrics(estimator, data, labels):   \n",
    "    print('Homogeneity: {}'.format(metrics.homogeneity_score(labels, estimator.labels_)))\n",
    "    print('Inertia: {}'.format(estimator.inertia_))\n",
    "    print('Number of Clusters: {}'.format(estimator.n_clusters))\n",
    "    \n",
    "    \n",
    "clusters = [10]\n",
    "for n_clusters in clusters:\n",
    "    estimator = MiniBatchKMeans(n_clusters = n_clusters)\n",
    "    estimator.fit(x)\n",
    "    calculate_metrics(estimator, x, y_train)\n",
    "    cluster_labels = infer_cluster_labels(estimator, y_train)\n",
    "    predicted_Y = infer_data_labels(estimator.labels_, cluster_labels)\n",
    "    print('KMeans Accuracy: {}\\n'.format(metrics.accuracy_score(y_train, predicted_Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing\n",
    "x_train=x_train.reshape(-1,28,28,1)/255\n",
    "x_test=x_test.reshape(-1,28,28,1)/255\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auto Encoder using K-Means clustering\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "model = Sequential()\n",
    "model.add(Conv2D(14, kernel_size=3, padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "model.add(MaxPool2D((2,2), padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(7, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPool2D((2,2), padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(7, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(UpSampling2D((2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(14, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(UpSampling2D((2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(1, kernel_size=3, padding='same', activation='relu'))\n",
    "model.compile(optimizer=SGD(0.01,0.9), loss=\"mse\", metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_AE=model.fit(x_train, x_train, epochs=10, batch_size=256, validation_data=(x_validate, x_validate), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossPlot():\n",
    "    trainingLoss,=plt.plot(history_AE.history['loss'],\"r--\")\n",
    "    validationLoss,=plt.plot(history_AE.history['val_loss'],\"b--\")\n",
    "    q=plt.legend([trainingLoss,validationLoss],[\"Training Loss\",\"Validation Loss\"])\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss & Validation Loss vs Number of Epochs Graph\")\n",
    "    plt.show()\n",
    "lossPlot()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
